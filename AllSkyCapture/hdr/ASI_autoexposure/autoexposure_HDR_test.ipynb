{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***High Dynamic Range Imaging***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import log2,ceil\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from IPython.display import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images and exposure times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all pathnames of exposure scaled images. We load input images and exposure times from same folder as this notebook. The filename of the jpg images has the exposure time (in &mu;s) encoded into it.<br>\n",
    "Read list of image file names and exposures into list, split the file name and exposure and read images into array images and exposure times into array times. The images are automatically converted into 8 bit during read with cv.imread, if not -1 is specified as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "times = []\n",
    "path = os.getcwd()\n",
    "files=glob.glob(\"analemma*.png\")\n",
    "for file in files:\n",
    "    images.append(cv.imread(os.path.join(path, file),-1))\n",
    "    times.append(int(file.split('.')[0].split('_')[-1]))\n",
    "times = np.asarray(times, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get info on image parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Channels: 3 / Datatype: uint16\n",
      "# of distinct levels: 53625 (2^16=65536)\n"
     ]
    }
   ],
   "source": [
    "print('# of Channels: %d / Datatype: %s' % (images[1].shape[2], str(images[1].dtype)))\n",
    "unique, counts = np.unique(images[1].flatten(), return_counts=True)\n",
    "counts = len(dict(zip(unique, counts)))\n",
    "bits = ceil(log2(counts))\n",
    "print('# of distinct levels: %d (2^%d=%d)' % ( counts, bits, 2**bits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate camera response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to know camera response function (CRF) for a lot of HDR construction algorithms. We use one of the calibration algorithms to estimate inverse CRF for all 256 pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.0-pre) /home/rainer/Documents/opencv/modules/photo/src/calibrate.cpp:74: error: (-215:Assertion failed) images[0].depth() == CV_8U in function 'process'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3bb47be5baaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcalibrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateCalibrateDebevec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibrate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.0-pre) /home/rainer/Documents/opencv/modules/photo/src/calibrate.cpp:74: error: (-215:Assertion failed) images[0].depth() == CV_8U in function 'process'\n"
     ]
    }
   ],
   "source": [
    "calibrate = cv.createCalibrateDebevec()\n",
    "response = calibrate.process(images, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make HDR image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Debevec's weighting scheme to construct HDR image using response calculated in the previous item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_debevec = cv.createMergeDebevec()\n",
    "hdr = merge_debevec.process(images, times, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tonemap HDR image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to see our results on common LDR display we have to map our HDR image to 8-bit range preserving most details. It is the main goal of tonemapping methods. We use tonemapper with bilateral filtering and set 2.2 as the value for gamma correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tonemap = cv.createTonemap(2.2)\n",
    "ldr = tonemap.process(hdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform exposure fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an alternative way to merge our exposures in case when we don't need HDR image. This process is called exposure fusion and produces LDR image that doesn't require gamma correction. It also doesn't use exposure values of the photographs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_mertens = cv.createMergeMertens()\n",
    "fusion = merge_mertens.process(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imwrite('fusion.png', fusion * 255)\n",
    "cv.imwrite('ldr.png', ldr * 255)\n",
    "cv.imwrite('hdr.hdr', hdr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to look at the results. Note that HDR image can't be stored in one of common image formats, so we save it to Radiance image (.hdr). Also all HDR imaging functions return results in [0, 1] range so we should multiply result by 255.\n",
    "\n",
    "You can try other tonemap algorithms: [cv::TonemapDrago](https://docs.opencv.org/4.1.0/da/d53/classcv_1_1TonemapDrago.html), [cv::TonemapMantiuk](https://docs.opencv.org/4.1.0/de/d76/classcv_1_1TonemapMantiuk.html) and [cv::TonemapReinhard](https://docs.opencv.org/4.1.0/d0/dec/classcv_1_1TonemapReinhard.html) You can also adjust the parameters in the HDR calibration and tonemap methods for your own photos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tonemapped image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](ldr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exposure fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](fusion.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
